#!/bin/bash --login
#$ -cwd
#$ -l nvidia_v100            # Can instead use 'nvidia_a100' for the A100 GPUs (if permitted!)
run_command() {
    local data=$1

    # Use command based on dataset
    if [[ $data == "wikipedia" ]]; then
        python train_self_supervised.py --use_memory --n_runs 10 --n_epoch 70 --n_head 4 --n_layer 2
    elif [[ $data == "reddit" ]]; then
        python train_self_supervised.py -d reddit --use_memory --n_runs 10 --n_epoch 70 --n_layer 1 --beta 0.00001
    elif [[ $data == "mooc" ]]; then
        python train_self_supervised.py --data mooc --use_memory --n_runs 10 --n_epoch 70 --n_layer 2
    elif [[ $data == "lastfm" ]]; then
        python train_self_supervised.py --data lastfm --use_memory --n_runs 10 --n_epoch 70 --n_layer 2 -pd 64 -ped 128 --scheduler 25
    elif [[ $data == "wikipedia_fm" ]]; then
        python train_self_supervised.py -d wikipedia_fm --use_memory --n_runs 10 --n_epoch 70 --n_head 4 --n_layer 2 -pd 64 -ped 128 --scheduler 20
    elif [[ $data == "reddit_fm" ]]; then
        python train_self_supervised.py -d reddit_fm --use_memory --n_runs 10 --n_epoch 70 --n_head 4 --n_layer 2 -pd 64 -ped 128 --scheduler 20
    fi
}

module load libs/cuda/11.7.0

# Copy a directory of files from scratch to the GPU node's local NVMe storage
cp -r ~/scratch/papers/TPPGN/ $TMPDIR

# Process the data with a GPU app, from within the local NVMe storage area
cd $TMPDIR/TPPGN/
run_command $1

# Copy the results back to the main scratch area
rsync -av $TMPDIR/TPPGN/log/ ~/scratch/papers/TPPGN/log/
rsync -av $TMPDIR/TPPGN/results/ ~/scratch/papers/TPPGN/results/
rsync -av $TMPDIR/TPPGN/saved_checkpoints/ ~/scratch/papers/TPPGN/saved_checkpoints/
rsync -av $TMPDIR/TPPGN/saved_models/ ~/scratch/papers/TPPGN/saved_models/

# The batch system will automatically delete the contents of $TMPDIR at the end of your job.
cd ~/scratch/papers/TPPGN/
git pull
git add ./log/
git commit -m "logs updated"
git push

sleep 3
#$ -m ea
#$ -M jiafeng.xiong@manchester.ac.uk     # Send email when finished.